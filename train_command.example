PYTHONPATH=. torchrun --nproc-per-node=4 \
gpt2/train.py --config-name=gpt2_pretraining \
data.batch_size=16 \
optimizer.gradient_accumulation_steps=16 \
optimizer.max_iters=300000 \
data.sequence_length=4096 \
dataset.extra_datasets="['epr-labs/maestro-augmented', 'epr-labs/giant-midi-augmented', 'epr-labs/pianofor-ai-augmented', 'epr-labs/imslp-augmented', 'epr-labs/pijamia-midi-v1', 'epr-labs/lakh-lmd-full']" \
dataset.augmentation.max_pitch_shift=0 \
"dataset.augmentation.speed_change_factors=[]" \
lr.warmup_iters=1000 \
lr.learning_rate=8e-5 \
lr.min_lr=8e-6 \
model=gpt2_medium \
model.n_embd=1024 \
model.n_head=8 \
model.n_layer=8 \
system.data_workers=64 \
system.compile=true \
loss_masking=pretrianing \
init_from=scratch \
logging.wandb_run_name_suffix=pretrianing \
