defaults:
  - model: gpt2 # "gpt2_minimal", "gpt2_medium", "gpt2_large", "gpt2_xl"
  - data: default
  - tokenizer: exponential
  - dataset: augmented
  - tasks: subsequence
  - metrics: default
  - logging: eval
  - _self_

eval_iters: 1

dataset:
  extra_datasets: ["epr-labs/maestro-sustain-v2"]
  augmentation:
    max_pitch_shift: 0
    speed_change_factors: []

init_from: midi-gpt2-3M-subsequence-2024-11-25-07-48.pt # should be 'midi-gpt2*'

task: multi_with_composer

loss_masking: pretraining

system:
  device: 'cuda'  # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks
  dtype: 'float16'  # 'float32', 'bfloat16', or 'float16', the latter will auto implement a GradScaler
  compile: true  # use PyTorch 2.0 to compile the model to be faster
  data_workers: 8
