#302M parameters with vocab_size=219
n_layer: 24
n_head: 16
n_embd: 1024
dropout: 0.0  # for pretraining 0 is good, for finetuning try 0.1+
bias: false  # do we use bias inside LayerNorm and Linear layers?
context_size: ${training.context_size}
